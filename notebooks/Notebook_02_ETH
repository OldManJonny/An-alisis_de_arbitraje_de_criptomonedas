{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEINyiDZ0Pd6TVmPFFmInZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Notebook para procesar ETH**"],"metadata":{"id":"vLEkMuv-AEFu"}},{"cell_type":"code","source":["# 1. Importar las bibliotecas necesarias\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from datetime import datetime\n","import os\n","import glob\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"NZJmIqwQ__q1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Montar Google Drive y crear estructura de carpetas\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"betYTm3RAbdu","executionInfo":{"status":"ok","timestamp":1744749861828,"user_tz":360,"elapsed":905,"user":{"displayName":"Jonathan Ayala","userId":"05013033828997019963"}},"outputId":"2594135a-f6ea-45c5-aabd-41b347e835ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Crear estructura de carpetas\n","!mkdir -p data/raw\n","!mkdir -p data/processed\n","!mkdir -p visualizations\n","\n","print(\"Estructura de carpetas creada exitosamente.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNSy3uM0AdyH","executionInfo":{"status":"ok","timestamp":1744749862774,"user_tz":360,"elapsed":352,"user":{"displayName":"Jonathan Ayala","userId":"05013033828997019963"}},"outputId":"e6b9362b-ae94-4104-d6bd-2a2fffb95a64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Estructura de carpetas creada exitosamente.\n"]}]},{"cell_type":"code","source":["# 3. Función para cargar datos de ETH\n","def load_eth_data():\n","    \"\"\"\n","    Función específica para cargar solo datos de ETH con mejor manejo de errores\n","    \"\"\"\n","    # Ruta al directorio de ETH\n","    eth_path = '/content/drive/My Drive/Cripto/Proyecto_Exchange/Blockchain/ETH'\n","\n","    print(f\"Buscando archivos CSV para ETH en: {eth_path}\")\n","\n","    # Verificar si la carpeta existe\n","    if not os.path.exists(eth_path):\n","        print(f\"¡ERROR! La carpeta {eth_path} no existe.\")\n","\n","        # Intentar listar directorios superiores para ayudar a depurar\n","        parent_dir = '/content/drive/My Drive/Cripto/Proyecto_Exchange/Blockchain'\n","        if os.path.exists(parent_dir):\n","            print(f\"El directorio padre existe: {parent_dir}\")\n","            print(\"Contenido:\")\n","            !ls \"{parent_dir}\"\n","        else:\n","            print(f\"El directorio padre NO existe: {parent_dir}\")\n","\n","            # Intentar subir un nivel más\n","            parent_parent = '/content/drive/My Drive/Cripto/Proyecto_Exchange'\n","            if os.path.exists(parent_parent):\n","                print(f\"Directorio encontrado: {parent_parent}\")\n","                print(\"Contenido:\")\n","                !ls \"{parent_parent}\"\n","            else:\n","                print(f\"No se encontró: {parent_parent}\")\n","\n","                # Último intento\n","                cripto_dir = '/content/drive/My Drive/Cripto'\n","                if os.path.exists(cripto_dir):\n","                    print(f\"Directorio Cripto encontrado: {cripto_dir}\")\n","                    print(\"Contenido:\")\n","                    !ls \"{cripto_dir}\"\n","                else:\n","                    print(\"Revisa tu estructura de directorios en Google Drive\")\n","\n","        return None\n","\n","    # Listar archivos CSV\n","    csv_files = glob.glob(os.path.join(eth_path, \"*.csv\"))\n","\n","    if not csv_files:\n","        print(f\"No se encontraron archivos CSV en {eth_path}\")\n","        print(\"Contenido del directorio:\")\n","        !ls \"{eth_path}\"\n","        return None\n","\n","    print(f\"Encontrados {len(csv_files)} archivos CSV\")\n","    for file in csv_files:\n","        print(f\"  - {os.path.basename(file)}\")\n","\n","    # Lista para almacenar DataFrames\n","    all_dfs = []\n","\n","    # Procesar cada archivo\n","    for file_path in csv_files:\n","        file_name = os.path.basename(file_path)\n","        print(f\"Procesando {file_name}...\")\n","\n","        # Determinar el exchange\n","        if 'Binance' in file_name:\n","            exchange = 'Binance'\n","        elif 'Coinbase' in file_name:\n","            exchange = 'Coinbase'\n","        elif 'Kucoin' in file_name or 'KuCoin' in file_name:\n","            exchange = 'Kucoin'\n","        elif 'OKX' in file_name:\n","            exchange = 'OKX'\n","        else:\n","            print(f\"No se pudo identificar el exchange para {file_name}, saltando...\")\n","            continue\n","\n","        try:\n","            # Leer solo unas pocas filas para examinar\n","            sample = pd.read_csv(file_path, nrows=5)\n","            print(f\"Columnas encontradas: {sample.columns.tolist()}\")\n","\n","            # Determinar columna de tiempo\n","            timestamp_col = None\n","            if 'Open time' in sample.columns:\n","                timestamp_col = 'Open time'\n","                sample_value = str(sample[timestamp_col].iloc[0])\n","                print(f\"Muestra de timestamp: {sample_value}\")\n","\n","            # Cargar los datos (aumentando a 1,000,000 filas para tener más datos)\n","            # Para asegurarnos de obtener los datos más recientes, podríamos leer todo y quedarnos con las últimas filas\n","            df = pd.read_csv(file_path, nrows=1000000)\n","            print(f\"Datos cargados: {df.shape}\")\n","\n","            # Ordenar por timestamp y quedarnos con los datos más recientes\n","            if 'Open time' in df.columns:\n","                try:\n","                    df['temp_time'] = pd.to_datetime(df['Open time'], errors='coerce')\n","                    df = df.sort_values('temp_time', ascending=False)\n","                    df = df.head(1000000)  # Nos quedamos con las filas más recientes\n","                    df = df.drop('temp_time', axis=1)\n","                except Exception as e:\n","                    print(f\"No se pudo ordenar por tiempo: {e}\")\n","\n","            # Añadir información de exchange y criptomoneda\n","            df['exchange'] = exchange\n","            df['cryptocurrency'] = 'ETH'  # Fijamos a ETH\n","\n","            # Manejar timestamp\n","            if timestamp_col:\n","                try:\n","                    # Intentar convertir a datetime\n","                    df['timestamp'] = pd.to_datetime(df[timestamp_col])\n","                except:\n","                    try:\n","                        # Si falla, intentar como timestamp unix\n","                        df['timestamp'] = pd.to_datetime(df[timestamp_col], unit='ms')\n","                    except:\n","                        print(f\"Error al convertir timestamp. Creando artificialmente.\")\n","                        # Crear timestamps artificiales\n","                        start_date = pd.Timestamp('2017-01-01')\n","                        df['timestamp'] = [start_date + pd.Timedelta(minutes=i) for i in range(len(df))]\n","            else:\n","                # Si no hay columna de timestamp, crear artificialmente\n","                start_date = pd.Timestamp('2017-01-01')\n","                df['timestamp'] = [start_date + pd.Timedelta(minutes=i) for i in range(len(df))]\n","\n","            # Estandarizar nombres de columnas\n","            col_mapping = {\n","                'Open': 'open',\n","                'High': 'high',\n","                'Low': 'low',\n","                'Close': 'close',\n","                'Volume': 'volume'\n","            }\n","\n","            for old_col, new_col in col_mapping.items():\n","                if old_col in df.columns:\n","                    df[new_col] = df[old_col]\n","\n","            # Verificar columnas mínimas\n","            required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n","            for col in required_cols:\n","                if col not in df.columns:\n","                    print(f\"Falta columna {col}. Intentando crear...\")\n","                    if col in ['open', 'high', 'low'] and 'close' in df.columns:\n","                        df[col] = df['close']\n","                    elif col == 'volume':\n","                        df[col] = 0\n","\n","            all_dfs.append(df)\n","            print(f\"Cargado exitosamente: {len(df)} filas\")\n","\n","        except Exception as e:\n","            print(f\"Error al cargar {file_name}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    # Combinar todos los DataFrames\n","    if all_dfs:\n","        combined_df = pd.concat(all_dfs, ignore_index=True)\n","        print(f\"Dataset combinado creado con {len(combined_df)} filas.\")\n","\n","        # Guardar dataset crudo\n","        combined_df.to_csv('data/raw/eth_raw.csv', index=False)\n","        print(\"Dataset crudo guardado en 'data/raw/eth_raw.csv'\")\n","\n","        return combined_df\n","    else:\n","        print(\"No se pudieron cargar datasets.\")\n","        return None\n"],"metadata":{"id":"5ISIiRrrArOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Función para limpiar datos\n","def clean_data(df):\n","    \"\"\"Limpia y prepara los datos\"\"\"\n","    print(\"Limpiando datos...\")\n","\n","    # Verificar que el DataFrame no es None\n","    if df is None:\n","        print(\"Error: DataFrame es None\")\n","        return None\n","\n","    # Crear copia\n","    df_clean = df.copy()\n","\n","    # Asegurar que timestamp es datetime\n","    if 'timestamp' in df_clean.columns:\n","        df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n","    else:\n","        print(\"Error: No hay columna 'timestamp'\")\n","        return None\n","\n","    # Convertir columnas numéricas\n","    for col in ['open', 'high', 'low', 'close', 'volume']:\n","        if col in df_clean.columns:\n","            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n","\n","    # Eliminar duplicados\n","    df_clean.drop_duplicates(subset=['timestamp', 'exchange'], inplace=True)\n","\n","    # Eliminar filas con NaN en columnas críticas\n","    df_clean.dropna(subset=['timestamp', 'close'], inplace=True)\n","\n","    # Ordenar\n","    df_clean.sort_values(['exchange', 'timestamp'], inplace=True)\n","\n","    print(f\"Limpieza completada: {len(df_clean)} filas restantes\")\n","    return df_clean"],"metadata":{"id":"YyQzmUHlAvL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. Función para sincronizar timestamps\n","def sync_timestamps(df):\n","    \"\"\"Sincroniza los timestamps entre exchanges\"\"\"\n","    print(\"Sincronizando timestamps...\")\n","\n","    # Verificar que el DataFrame no es None\n","    if df is None:\n","        print(\"Error: DataFrame es None\")\n","        return None\n","\n","    # Verificar que hay suficientes exchanges\n","    exchanges = df['exchange'].unique()\n","    if len(exchanges) < 2:\n","        print(f\"Error: Se necesitan al menos 2 exchanges, pero solo hay {len(exchanges)}: {exchanges}\")\n","        return None\n","\n","    try:\n","        # Encontrar rango común de tiempo\n","        min_times = df.groupby('exchange')['timestamp'].min()\n","        max_times = df.groupby('exchange')['timestamp'].max()\n","\n","        common_start = min_times.max()\n","        common_end = max_times.min()\n","\n","        print(f\"Rango común: {common_start} a {common_end}\")\n","\n","        # Si el rango común es muy pequeño o no existe, usar un enfoque diferente\n","        if common_start >= common_end:\n","            print(\"Advertencia: No hay suficiente superposición entre los exchanges.\")\n","            print(\"Intentando un enfoque alternativo...\")\n","\n","            # Encontrar el exchange con menos datos\n","            counts = df.groupby('exchange').size()\n","            min_exchange = counts.idxmin()\n","\n","            # Usar el rango de tiempo de ese exchange\n","            exchange_df = df[df['exchange'] == min_exchange]\n","            alt_start = exchange_df['timestamp'].min()\n","            alt_end = exchange_df['timestamp'].max()\n","\n","            print(f\"Usando rango de {min_exchange}: {alt_start} a {alt_end}\")\n","            df_filtered = df[(df['timestamp'] >= alt_start) & (df['timestamp'] <= alt_end)]\n","        else:\n","            # Filtrar al rango común\n","            df_filtered = df[(df['timestamp'] >= common_start) & (df['timestamp'] <= common_end)]\n","\n","        # Verificar que hay datos después de filtrar\n","        if len(df_filtered) == 0:\n","            print(\"Error: No hay datos en el rango común\")\n","            return None\n","\n","        print(f\"Filas después de filtrar por rango: {len(df_filtered)}\")\n","\n","        # Sincronizar cada exchange\n","        synced_dfs = []\n","\n","        for exchange in exchanges:\n","            exchange_df = df_filtered[df_filtered['exchange'] == exchange].copy()\n","\n","            # Verificar que hay datos para este exchange\n","            if len(exchange_df) == 0:\n","                print(f\"Advertencia: No hay datos para {exchange} en el rango común\")\n","                continue\n","\n","            print(f\"Sincronizando {exchange}: {len(exchange_df)} filas\")\n","\n","            # Convertir timestamp a índice\n","            exchange_df.set_index('timestamp', inplace=True)\n","\n","            # Resamplear a intervalos de 1 minuto\n","            try:\n","                resampled = exchange_df.resample('1min').agg({\n","                    'open': 'first',\n","                    'high': 'max',\n","                    'low': 'min',\n","                    'close': 'last',\n","                    'volume': 'sum',\n","                    'exchange': 'first',\n","                    'cryptocurrency': 'first'\n","                })\n","\n","                # Resetear índice\n","                resampled.reset_index(inplace=True)\n","\n","                # Verificar si hay muchos valores NaN después del resampling\n","                na_percentage = resampled['close'].isna().mean() * 100\n","                if na_percentage > 50:\n","                    print(f\"Advertencia: {na_percentage:.2f}% de valores NaN para {exchange} después del resampling\")\n","\n","                # Imputar valores NaN para que no causen problemas\n","                # Método de imputación forward fill seguido de backward fill\n","                resampled = resampled.fillna(method='ffill').fillna(method='bfill')\n","\n","                synced_dfs.append(resampled)\n","                print(f\"Resampling de {exchange} completado: {len(resampled)} filas\")\n","\n","            except Exception as e:\n","                print(f\"Error al resamplear {exchange}: {e}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","        # Combinar todos los exchanges sincronizados\n","        if synced_dfs:\n","            result = pd.concat(synced_dfs, ignore_index=True)\n","            print(f\"Sincronización completada: {len(result)} filas\")\n","\n","            # Verificar balance de datos entre exchanges\n","            exchange_counts = result.groupby('exchange').size()\n","            print(\"Distribución de filas por exchange:\")\n","            for ex, count in exchange_counts.items():\n","                print(f\"  - {ex}: {count} filas\")\n","\n","            return result\n","        else:\n","            print(\"Error en sincronización: No hay DataFrames para combinar\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"Error en sincronización: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None"],"metadata":{"id":"IMj5Lc3DAxe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6. Función para métricas de arbitraje\n","def calculate_arbitrage(df):\n","    \"\"\"Calcula métricas de arbitraje entre exchanges\"\"\"\n","    print(\"Calculando métricas de arbitraje...\")\n","\n","    # Verificar que el DataFrame no es None\n","    if df is None:\n","        print(\"Error: DataFrame es None\")\n","        return None\n","\n","    # Verificar que tenemos todas las columnas necesarias\n","    required_cols = ['timestamp', 'exchange', 'close', 'cryptocurrency']\n","    missing = [col for col in required_cols if col not in df.columns]\n","    if missing:\n","        print(f\"ERROR: Faltan columnas: {missing}\")\n","        print(f\"Columnas disponibles: {df.columns.tolist()}\")\n","        return None\n","\n","    try:\n","        # Verificar que hay suficientes exchanges\n","        exchanges = df['exchange'].unique()\n","        print(f\"Exchanges disponibles: {exchanges}\")\n","\n","        if len(exchanges) < 2:\n","            print(f\"Error: Se necesitan al menos 2 exchanges, pero solo hay {len(exchanges)}\")\n","            return None\n","\n","        # Verificar que hay datos suficientes\n","        exchange_counts = df.groupby('exchange').size()\n","        print(\"Recuentos por exchange:\")\n","        for ex, count in exchange_counts.items():\n","            print(f\"  - {ex}: {count} filas\")\n","            if count < 10:\n","                print(f\"    ¡Advertencia! Muy pocas filas para {ex}\")\n","\n","        # Pivot para obtener precios por exchange\n","        try:\n","            pivot_df = df.pivot_table(\n","                index='timestamp',\n","                columns='exchange',\n","                values='close'\n","            )\n","\n","            print(f\"Datos pivotados: {pivot_df.shape}\")\n","            print(f\"Exchanges en pivot: {pivot_df.columns.tolist()}\")\n","\n","            # Verificar valores nulos\n","            null_pcts = pivot_df.isnull().mean() * 100\n","            print(\"Porcentaje de valores nulos por exchange:\")\n","            for ex, pct in null_pcts.items():\n","                print(f\"  - {ex}: {pct:.2f}%\")\n","\n","            # Imputar valores nulos para evitar problemas en los cálculos\n","            # Primero forward fill, luego backward fill\n","            pivot_df = pivot_df.fillna(method='ffill').fillna(method='bfill')\n","\n","            # Calcular spreads\n","            exchanges = pivot_df.columns\n","\n","            for i, exchange1 in enumerate(exchanges):\n","                for exchange2 in exchanges[i+1:]:\n","                    # Verificar que hay datos suficientes para ambos exchanges\n","                    if pivot_df[exchange1].isna().all() or pivot_df[exchange2].isna().all():\n","                        print(f\"Advertencia: No hay datos suficientes para {exchange1} o {exchange2}\")\n","                        continue\n","\n","                    # Spread absoluto\n","                    spread_col = f'spread_{exchange1}_{exchange2}'\n","                    pivot_df[spread_col] = pivot_df[exchange1] - pivot_df[exchange2]\n","\n","                    # Spread relativo (con manejo de división por cero)\n","                    rel_spread_col = f'rel_spread_{exchange1}_{exchange2}'\n","                    # Evitar división por cero o valores muy pequeños\n","                    pivot_df[rel_spread_col] = (\n","                        (pivot_df[exchange1] - pivot_df[exchange2]) /\n","                        pivot_df[exchange2].replace(0, float('nan')) * 100\n","                    )\n","\n","                    # Limpiar valores extremos (más de ±100%) que podrían ser errores\n","                    extreme_mask = (pivot_df[rel_spread_col].abs() > 100)\n","                    extreme_count = extreme_mask.sum()\n","                    if extreme_count > 0:\n","                        print(f\"Se encontraron {extreme_count} spreads extremos (>100%) para {exchange1}-{exchange2}\")\n","                        # Reemplazar con NaN los valores extremos\n","                        pivot_df.loc[extreme_mask, rel_spread_col] = np.nan\n","\n","                    # Oportunidades (>0.5%)\n","                    pivot_df[f'arbitrage_opp_{exchange1}_{exchange2}'] = (\n","                        pivot_df[rel_spread_col].abs() > 0.5\n","                    ).astype(int)\n","\n","                    # Contar oportunidades\n","                    opp_count = pivot_df[f'arbitrage_opp_{exchange1}_{exchange2}'].sum()\n","                    print(f\"Par {exchange1}-{exchange2}: {opp_count} oportunidades de arbitraje\")\n","\n","            # Añadir métricas adicionales\n","            pivot_df['avg_price'] = pivot_df[exchanges].mean(axis=1)\n","\n","            # Resetear índice\n","            pivot_df.reset_index(inplace=True)\n","\n","            # Añadir columna de criptomoneda\n","            pivot_df['cryptocurrency'] = 'ETH'\n","\n","            print(f\"Métricas calculadas: {len(pivot_df)} filas\")\n","\n","            # Verificar que se han generado columnas de arbitraje\n","            arb_cols = [col for col in pivot_df.columns if col.startswith('arbitrage_opp_')]\n","            if len(arb_cols) == 0:\n","                print(\"¡Advertencia! No se generaron columnas de oportunidades de arbitraje\")\n","            else:\n","                print(f\"Se generaron {len(arb_cols)} métricas de arbitraje\")\n","\n","            return pivot_df\n","\n","        except Exception as e:\n","            print(f\"Error en el pivot o cálculo de métricas: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","            return None\n","\n","    except Exception as e:\n","        print(f\"Error al calcular métricas: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None"],"metadata":{"id":"D9yCdIwnA0Xo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. Función principal\n","def process_eth():\n","    \"\"\"Función principal para procesar ETH\"\"\"\n","    print(\"\\n=== PROCESANDO ETH ===\\n\")\n","\n","    # Inicializar variables\n","    clean_df = None\n","    sync_df = None\n","    arb_df = None\n","\n","    try:\n","        # 1. Cargar datos\n","        print(\"\\n----- CARGANDO DATOS -----\")\n","        raw_data = load_eth_data()\n","\n","        if raw_data is None or len(raw_data) == 0:\n","            print(\"No se pudieron cargar datos. Abortando.\")\n","            return None, None, None\n","\n","        # 2. Limpiar datos\n","        print(\"\\n----- LIMPIANDO DATOS -----\")\n","        clean_df = clean_data(raw_data)\n","\n","        if clean_df is None or len(clean_df) == 0:\n","            print(\"Error en la limpieza de datos. Abortando.\")\n","            return None, None, None\n","\n","        # Guardar datos limpios\n","        clean_df.to_csv('data/processed/eth_clean.csv', index=False)\n","        print(\"Datos limpios guardados en 'data/processed/eth_clean.csv'\")\n","\n","        # Liberar memoria\n","        del raw_data\n","        gc.collect()\n","\n","        # 3. Sincronizar timestamps\n","        print(\"\\n----- SINCRONIZANDO TIMESTAMPS -----\")\n","        sync_df = sync_timestamps(clean_df)\n","\n","        if sync_df is None or len(sync_df) == 0:\n","            print(\"Error en la sincronización. Abortando.\")\n","            return clean_df, None, None\n","\n","        # Guardar datos sincronizados\n","        sync_df.to_csv('data/processed/eth_synchronized.csv', index=False)\n","        print(\"Datos sincronizados guardados en 'data/processed/eth_synchronized.csv'\")\n","\n","        # 4. Calcular métricas de arbitraje\n","        print(\"\\n----- CALCULANDO MÉTRICAS DE ARBITRAJE -----\")\n","        arb_df = calculate_arbitrage(sync_df)\n","\n","        if arb_df is None or len(arb_df) == 0:\n","            print(\"Error en el cálculo de métricas. Abortando.\")\n","            return clean_df, sync_df, None\n","\n","        # Guardar métricas\n","        arb_df.to_csv('data/processed/eth_arbitrage.csv', index=False)\n","        print(\"Métricas de arbitraje guardadas en 'data/processed/eth_arbitrage.csv'\")\n","\n","        # 5. Generar visualización simple\n","        print(\"\\n----- GENERANDO VISUALIZACIÓN -----\")\n","        try:\n","            # Comparación de precios\n","            pivot_df = sync_df.pivot_table(\n","                index='timestamp',\n","                columns='exchange',\n","                values='close'\n","            )\n","\n","            fig = go.Figure()\n","\n","            for exchange in pivot_df.columns:\n","                fig.add_trace(go.Scatter(\n","                    x=pivot_df.index,\n","                    y=pivot_df[exchange],\n","                    mode='lines',\n","                    name=exchange\n","                ))\n","\n","            fig.update_layout(\n","                title='Comparación de Precios de ETH entre Exchanges',\n","                xaxis_title='Fecha',\n","                yaxis_title='Precio (USD)'\n","            )\n","\n","            fig.write_html('visualizations/eth_price_comparison.html')\n","            print(\"Visualización guardada en 'visualizations/eth_price_comparison.html'\")\n","\n","            # Visualización de oportunidades de arbitraje\n","            # Identificar columnas de oportunidades\n","            arb_cols = [col for col in arb_df.columns if col.startswith('arbitrage_opp_')]\n","\n","            # Crear columna con total de oportunidades\n","            arb_df['total_opportunities'] = arb_df[arb_cols].sum(axis=1)\n","\n","            # Crear gráfico de línea\n","            fig2 = px.line(\n","                arb_df,\n","                x='timestamp',\n","                y='total_opportunities',\n","                title='Oportunidades de Arbitraje para ETH',\n","                labels={'timestamp': 'Fecha', 'total_opportunities': 'Número de Oportunidades'}\n","            )\n","\n","            fig2.write_html('visualizations/eth_arbitrage_opportunities.html')\n","            print(\"Visualización de arbitraje guardada en 'visualizations/eth_arbitrage_opportunities.html'\")\n","\n","        except Exception as e:\n","            print(f\"Error al generar visualizaciones: {e}\")\n","            # Continuamos aunque falle la visualización\n","\n","        print(\"\\n=== PROCESAMIENTO COMPLETADO EXITOSAMENTE ===\\n\")\n","\n","    except Exception as e:\n","        print(f\"Error general en el procesamiento: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    return clean_df, sync_df, arb_df"],"metadata":{"id":"U7QC1YQXA234"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ejecutar el procesamiento\n","clean_df, sync_df, arb_df = process_eth()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vnEWlQ2A5mB","executionInfo":{"status":"ok","timestamp":1744750141548,"user_tz":360,"elapsed":258652,"user":{"displayName":"Jonathan Ayala","userId":"05013033828997019963"}},"outputId":"4417dcc7-c5e8-439e-8424-854be30ba879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== PROCESANDO ETH ===\n","\n","\n","----- CARGANDO DATOS -----\n","Buscando archivos CSV para ETH en: /content/drive/My Drive/Cripto/Proyecto_Exchange/Blockchain/ETH\n","Encontrados 4 archivos CSV\n","  - ETHUSD_1m_Coinbase.csv\n","  - ETHUSD_1m_KuCoin.csv\n","  - ETHUSD_1m_Binance.csv\n","  - ETHUSD_1m_OKX.csv\n","Procesando ETHUSD_1m_Coinbase.csv...\n","Columnas encontradas: ['Open time', 'Low', 'High', 'Open', 'Close', 'Volume']\n","Muestra de timestamp: 2016-09-29 00:00:00\n","Datos cargados: (1000000, 6)\n","Cargado exitosamente: 1000000 filas\n","Procesando ETHUSD_1m_KuCoin.csv...\n","Columnas encontradas: ['Open time', 'Open', 'Close', 'High', 'Low', 'Volume', 'Amount']\n","Muestra de timestamp: 2018-01-01 00:13:00\n","Datos cargados: (1000000, 7)\n","Cargado exitosamente: 1000000 filas\n","Procesando ETHUSD_1m_Binance.csv...\n","Columnas encontradas: ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close time', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore']\n","Muestra de timestamp: 2017-08-17 04:00:00\n","Datos cargados: (1000000, 12)\n","Cargado exitosamente: 1000000 filas\n","Procesando ETHUSD_1m_OKX.csv...\n","Columnas encontradas: ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Volume (Currency)', 'Volume (Quote)', 'Confirm']\n","Muestra de timestamp: 2018-01-11 11:12:00\n","Datos cargados: (1000000, 9)\n","Cargado exitosamente: 1000000 filas\n","Dataset combinado creado con 4000000 filas.\n","Dataset crudo guardado en 'data/raw/eth_raw.csv'\n","\n","----- LIMPIANDO DATOS -----\n","Limpiando datos...\n","Limpieza completada: 4000000 filas restantes\n","Datos limpios guardados en 'data/processed/eth_clean.csv'\n","\n","----- SINCRONIZANDO TIMESTAMPS -----\n","Sincronizando timestamps...\n","Rango común: 2018-01-11 11:12:00 a 2018-11-03 02:39:00\n","Filas después de filtrar por rango: 1516087\n","Sincronizando Binance: 422296 filas\n","Resampling de Binance completado: 425728 filas\n","Sincronizando Coinbase: 423508 filas\n","Resampling de Coinbase completado: 425728 filas\n","Sincronizando Kucoin: 244555 filas\n","Resampling de Kucoin completado: 425727 filas\n","Sincronizando OKX: 425728 filas\n","Resampling de OKX completado: 425728 filas\n","Sincronización completada: 1702911 filas\n","Distribución de filas por exchange:\n","  - Binance: 425728 filas\n","  - Coinbase: 425728 filas\n","  - Kucoin: 425727 filas\n","  - OKX: 425728 filas\n","Datos sincronizados guardados en 'data/processed/eth_synchronized.csv'\n","\n","----- CALCULANDO MÉTRICAS DE ARBITRAJE -----\n","Calculando métricas de arbitraje...\n","Exchanges disponibles: ['Binance' 'Coinbase' 'Kucoin' 'OKX']\n","Recuentos por exchange:\n","  - Binance: 425728 filas\n","  - Coinbase: 425728 filas\n","  - Kucoin: 425727 filas\n","  - OKX: 425728 filas\n","Datos pivotados: (425728, 4)\n","Exchanges en pivot: ['Binance', 'Coinbase', 'Kucoin', 'OKX']\n","Porcentaje de valores nulos por exchange:\n","  - Binance: 0.00%\n","  - Coinbase: 0.00%\n","  - Kucoin: 0.00%\n","  - OKX: 0.00%\n","Par Binance-Coinbase: 91051 oportunidades de arbitraje\n","Par Binance-Kucoin: 48981 oportunidades de arbitraje\n","Par Binance-OKX: 9952 oportunidades de arbitraje\n","Par Coinbase-Kucoin: 121876 oportunidades de arbitraje\n","Par Coinbase-OKX: 86730 oportunidades de arbitraje\n","Par Kucoin-OKX: 50886 oportunidades de arbitraje\n","Métricas calculadas: 425728 filas\n","Se generaron 6 métricas de arbitraje\n","Métricas de arbitraje guardadas en 'data/processed/eth_arbitrage.csv'\n","\n","----- GENERANDO VISUALIZACIÓN -----\n","Visualización guardada en 'visualizations/eth_price_comparison.html'\n","Visualización de arbitraje guardada en 'visualizations/eth_arbitrage_opportunities.html'\n","\n","=== PROCESAMIENTO COMPLETADO EXITOSAMENTE ===\n","\n"]}]},{"cell_type":"code","source":["# Verificar resultados\n","if arb_df is not None:\n","    # Mostrar un resumen de las oportunidades de arbitraje\n","    arb_cols = [col for col in arb_df.columns if col.startswith('arbitrage_opp_')]\n","\n","    # Crear resumen de oportunidades por par de exchanges\n","    arb_summary = []\n","\n","    for col in arb_cols:\n","        # Extraer nombres de exchanges\n","        parts = col.split('_')\n","        exchange1 = parts[2]\n","        exchange2 = parts[3]\n","\n","        # Calcular total de oportunidades\n","        total_opps = arb_df[col].sum()\n","        pct_opps = (total_opps / len(arb_df)) * 100\n","\n","        # Calcular spread promedio\n","        spread_col = f'rel_spread_{exchange1}_{exchange2}'\n","        avg_spread = arb_df[spread_col].mean()\n","        max_spread = arb_df[spread_col].max()\n","\n","        arb_summary.append({\n","            'Par': f'{exchange1}-{exchange2}',\n","            'Oportunidades': total_opps,\n","            'Porcentaje': pct_opps,\n","            'Spread Promedio': avg_spread,\n","            'Spread Máximo': max_spread\n","        })\n","\n","    # Convertir a DataFrame y mostrar\n","    summary_df = pd.DataFrame(arb_summary)\n","    summary_df = summary_df.sort_values('Oportunidades', ascending=False)\n","\n","    print(\"\\nResumen de Oportunidades de Arbitraje:\")\n","    print(summary_df)\n","\n","    # Guardar resumen\n","    summary_df.to_csv('data/processed/eth_arbitrage_summary.csv', index=False)\n","    print(\"Resumen guardado en 'data/processed/eth_arbitrage_summary.csv'\")\n","else:\n","    print(\"\\nNo se generaron métricas de arbitraje.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBNHsHKgA7jA","executionInfo":{"status":"ok","timestamp":1744753301572,"user_tz":360,"elapsed":52,"user":{"displayName":"Jonathan Ayala","userId":"05013033828997019963"}},"outputId":"75c8b794-3e17-472d-ef01-6c2108167f5e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Resumen de Oportunidades de Arbitraje:\n","                Par  Oportunidades  Porcentaje  Spread Promedio  Spread Máximo\n","3   Coinbase-Kucoin         121876   28.627668        -0.259668      13.522936\n","0  Binance-Coinbase          91051   21.387130         0.238099      11.188678\n","4      Coinbase-OKX          86730   20.372163        -0.246837      12.256690\n","5        Kucoin-OKX          50886   11.952702         0.015488      10.509189\n","1    Binance-Kucoin          48981   11.505233        -0.032137       9.215332\n","2       Binance-OKX           9952    2.337643        -0.018630       5.764146\n","Resumen guardado en 'data/processed/eth_arbitrage_summary.csv'\n"]}]}]}